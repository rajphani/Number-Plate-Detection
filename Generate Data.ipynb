{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Generate Data.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"h4nF0NIFdQK1","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import cv2\n","import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KVqLLgMcdV-q","colab_type":"code","colab":{}},"cell_type":"code","source":["MIN_CONTOUR_AREA = 100\n","\n","RESIZED_IMAGE_WIDTH = 20\n","RESIZED_IMAGE_HEIGHT = 30"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3YZwG3gXRkpB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"695fa365-853c-42d1-baf2-291447488827","executionInfo":{"status":"ok","timestamp":1552204170388,"user_tz":-330,"elapsed":1256,"user":{"displayName":"Phaniraj Rallabandi","photoUrl":"https://lh3.googleusercontent.com/-3waliHg4XvU/AAAAAAAAAAI/AAAAAAAAA8g/3GJUn64Wb00/s64/photo.jpg","userId":"06780650722492889436"}}},"cell_type":"code","source":["imgTrainingNumbers = cv2.imread(\"training_chars.png\")  \n","print(imgTrainingNumbers)    # read in training numbers image"],"execution_count":8,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"metadata":{"id":"lseRIhXgdPKQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":251},"outputId":"b499835f-d732-49cc-d83a-d41610eea1a7","executionInfo":{"status":"error","timestamp":1552204080484,"user_tz":-330,"elapsed":1053,"user":{"displayName":"Phaniraj Rallabandi","photoUrl":"https://lh3.googleusercontent.com/-3waliHg4XvU/AAAAAAAAAAI/AAAAAAAAA8g/3GJUn64Wb00/s64/photo.jpg","userId":"06780650722492889436"}}},"cell_type":"code","source":["imgGray = cv2.cvtColor(imgTrainingNumbers, cv2.COLOR_BGR2GRAY)          # get grayscale image\n","imgBlurred = cv2.GaussianBlur(imgGray, (5,5), 0)                      # blur\n","                                                          \n","imgThresh = cv2.adaptiveThreshold(imgBlurred,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11, 2)                                    \n"],"execution_count":7,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-e5d7467f7cc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimgGray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgTrainingNumbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# get grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimgBlurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgGray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m                        \u001b[0;31m# blur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimgThresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptiveThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgBlurred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADAPTIVE_THRESH_GAUSSIAN_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY_INV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}]},{"metadata":{"id":"i4EJwGURlGnD","colab_type":"code","colab":{}},"cell_type":"code","source":["imgThreshCopy = imgThresh.copy()        # make a copy of the thresh image, this in necessary b/c findContours modifies the image\n","\n","imgContours, npaContours, npaHierarchy = cv2.findContours(imgThreshCopy,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)           # compress horizontal, vertical, and diagonal segments and leave only their end points\n","\n","                                # declare empty numpy array, we will use this to write to file later\n","                                # zero rows, enough cols to hold all image data\n","npaFlattenedImages =  np.empty((0, RESIZED_IMAGE_WIDTH * RESIZED_IMAGE_HEIGHT))\n","\n","intClassifications = []         # declare empty classifications list, this will be our list of how we are classifying our chars from user input, we will write to file at the end\n","\n","                                    # possible chars we are interested in are digits 0 through 9, put these in list intValidChars\n","intValidChars = [ord('0'), ord('1'), ord('2'), ord('3'), ord('4'), ord('5'), ord('6'), ord('7'), ord('8'), ord('9'),\n","                     ord('A'), ord('B'), ord('C'), ord('D'), ord('E'), ord('F'), ord('G'), ord('H'), ord('I'), ord('J'),\n","                     ord('K'), ord('L'), ord('M'), ord('N'), ord('O'), ord('P'), ord('Q'), ord('R'), ord('S'), ord('T'),\n","                     ord('U'), ord('V'), ord('W'), ord('X'), ord('Y'), ord('Z')]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ugdyz16Fl9yc","colab_type":"code","colab":{}},"cell_type":"code","source":["for npaContour in npaContours:                          # for each contour\n","  if cv2.contourArea(npaContour) > MIN_CONTOUR_AREA:          # if contour is big enough to consider\n","    [intX, intY, intW, intH] = cv2.boundingRect(npaContour)         # get and break out bounding rect\n","                                              # draw rectangle around each contour as we ask user for input\n","    cv2.rectangle(imgTrainingNumbers,(intX, intY), (intX+intW,intY+intH), (0, 0, 255), 2)                            # thickness\n","\n","    imgROI = imgThresh[intY:intY+intH, intX:intX+intW]                                  # crop char out of threshold image\n","    imgROIResized = cv2.resize(imgROI, (RESIZED_IMAGE_WIDTH, RESIZED_IMAGE_HEIGHT))     # resize image, this will be more consistent for recognition and storage\n","\n","    cv2.imshow(\"imgROI\", imgROI)                    # show cropped out char for reference\n","    cv2.imshow(\"imgROIResized\", imgROIResized)      # show resized image for reference\n","    cv2.imshow(\"training_numbers.png\", imgTrainingNumbers)      # show training numbers image, this will now have red rectangles drawn on it\n","\n","    intChar = cv2.waitKey(0)                     # get key press\n","\n","    if intChar == 27:                   # if esc key was pressed\n","      sys.exit()                      # exit program\n","    elif intChar in intValidChars:      # else if the char is in the list of chars we are looking for . . .\n","      intClassifications.append(intChar)                                                # append classification char to integer list of chars (we will convert to float later before writing to file)\n","      npaFlattenedImage = imgROIResized.reshape((1, RESIZED_IMAGE_WIDTH * RESIZED_IMAGE_HEIGHT))  # flatten image to 1d numpy array so we can write to file later\n","      npaFlattenedImages = np.append(npaFlattenedImages, npaFlattenedImage, 0)                    # add current flattened impage numpy array to list of flattened image numpy arrays\n","            # end if\n","        # end if\n","    # end for"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5oVEBz9smrD-","colab_type":"code","colab":{}},"cell_type":"code","source":["fltClassifications = np.array(intClassifications, np.float32)                   # convert classifications list of ints to numpy array of floats\n","\n","npaClassifications = fltClassifications.reshape((fltClassifications.size, 1))   # flatten numpy array of floats to 1d so we can write to file later\n","\n","print \"\\n\\ntraining complete !!\\n\"\n","np.savetxt(\"classifications.txt\", npaClassifications)           # write flattened images to file\n","np.savetxt(\"flattened_images.txt\", npaFlattenedImages)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"KCNcBt4YkqDF","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}